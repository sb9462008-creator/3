import sounddevice as sd
import librosa
import numpy as np
import matplotlib.pyplot as plt
import joblib
import os
from sklearn.ensemble import RandomForestClassifier

#  –î—É—É –±–∏—á–∏—Ö —Ñ—É–Ω–∫—Ü
def record_audio(duration=4, sr=22050):
    print(" –î—É—É –±–∏—á–∏–∂ –±–∞–π–Ω–∞...")
    recording = sd.rec(int(duration * sr), samplerate=sr, channels=1)
    sd.wait()
    print(" –î—É—É –±–∏—á–∏–≥–¥–ª—ç—ç!")
    return recording

#  MFCC —à–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö
def extract_mfcc(audio, sr=22050, n_mfcc=13):
    y = np.squeeze(audio)
    y, _ = librosa.effects.trim(y)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    mfcc_mean = np.mean(mfcc.T, axis=0).reshape(1, -1)
    return mfcc_mean

# üå≤ –ó–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞—Ö –±–∞ —Ö–∞–¥–≥–∞–ª–∞—Ö
def train_and_save_model(path="emotion_model.pkl"):
    X_train = np.random.rand(12, 13)
    y_train = np.array(["happy", "sad", "angry", "calm", "happy", "angry",
                        "sad", "happy", "calm", "angry", "sad", "happy"])
    model = RandomForestClassifier(n_estimators=150, random_state=42)
    model.fit(X_train, y_train)
    joblib.dump(model, path)
    print(f" –ó–∞–≥–≤–∞—Ä —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: {path}")

def load_model(path="emotion_model.pkl"):
    if not os.path.exists(path):
        train_and_save_model(path)
    return joblib.load(path)

#  –¢–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö
def predict_emotion(model, features):
    probabilities = model.predict_proba(features)[0]
    emotions = model.classes_
    best_emotion = emotions[np.argmax(probabilities)]
    return emotions, probabilities, best_emotion

#  –í–∏–∑—É–∞–ª “Ø—Ä –¥“Ø–Ω
def plot_emotions(emotions, probabilities):
    plt.figure(figsize=(8, 4))
    plt.bar(emotions, probabilities, color='skyblue')
    plt.title(" –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª")
    plt.ylabel("–ú–∞–≥–∞–¥–ª–∞–ª")
    plt.ylim(0, 1)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

#  “Æ–Ω–¥—Å—ç–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–≥—á
def main():
    model = load_model()
    audio = record_audio()
    features = extract_mfcc(audio)
    emotions, probabilities, best_emotion = predict_emotion(model, features)

    print("\n –¢–∞–Ω–∞–π –¥—É—É —Ö–æ–æ–ª–æ–π–Ω —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω —Ö—É–≤–∞–∞—Ä—å:")
    for emo, prob in zip(emotions, probabilities):
        print(f"  {emo.upper():<8}: {prob * 100:.2f}%")

    print(f"\n –ù–∏–π—Ç –¥“Ø–≥–Ω—ç–ª—Ç: {best_emotion.upper()} —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª –¥–∞–≤–∞–º–≥–∞–π –±–∞–π–Ω–∞.")
    plot_emotions(emotions, probabilities)

if __name__ == "__main__":
    main()